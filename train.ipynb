{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO('best.pt')  # load a .pt file\n",
    "\n",
    "# Use the model\n",
    "# model.train(data=\"config.yml\", epochs=100, amp=False)  # train the model for GPU\n",
    "# model.train(data=\"config.yml\", epochs=50)  # train the model for CPU\n",
    "# metrics = model.val()  # evaluate model performance on the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\jin-yee.chan\\Documents\\TaxtDetection\\test.jpg: 480x640 1 Tag, 72.0ms\n",
      "Speed: 9.2ms preprocess, 72.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from detect import Coords\n",
    "from PIL import Image\n",
    "imgPath = ('./test.jpg')\n",
    "# imgPath = ('./data/test/images/00047.jpg')\n",
    "\n",
    "results = model(imgPath)\n",
    "coords: list[Coords] = list()\n",
    "for result in results:\n",
    "    # print(result.boxes)\n",
    "    for box in result.boxes:\n",
    "        for xyxy in box.xyxy:\n",
    "            coord = Coords(xyxy[0], xyxy[1], xyxy[2], xyxy[3])\n",
    "            coords.append(coord)\n",
    "    # im_array = result.plot()  # plot a BGR numpy array of predictions\n",
    "    # im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "    # # im.show()  # show image\n",
    "    # im.save('results.jpg') \n",
    "# print(result[\"boxes\"])\n",
    "# import cv2\n",
    "# print(metrics)\n",
    "# path = model.export(format=\"onnx\") # export the model to ONNX format\n",
    "# print(path)\n",
    "# results = model(img)  # predict on an image\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coord in coords:\n",
    "    coord.drawBoundingBox(imgPath, saveImagePath='draw1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coord in coords:\n",
    "    # Crop image \n",
    "    coord.cropImage(imgPath, saveImagePath='crop.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([[494, 170], [2241, 170], [2241, 302], [494, 302]], 'V T _OE - 2 0 2 01 1 95 5', 0.3846795875551927)]\n"
     ]
    }
   ],
   "source": [
    "from recog import Recog\n",
    "recog = Recog()\n",
    "recog.preprocess('crop.jpg', \"binary.jpg\")\n",
    "recog.read('binary.jpg')\n",
    "print(recog.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V T _OE - 2 0 2 01 1 95 5\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "for result in recog.result:\n",
    "    print(result[1])\n",
    "    text += result[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VT_OE-202011955\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\" \", \"\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import cv2\n",
    "\n",
    "# Read one frame from the camera for testing\n",
    "video = cv2.VideoCapture(0)\n",
    "_, frame = video.read()\n",
    "video.release()\n",
    "\n",
    "import re\n",
    "from timeit import timeit\n",
    "import math\n",
    "\n",
    "extensions=re.findall(r\"\\\\\\*(\\.\\w*)\", cv2.imread.__doc__)\n",
    "\n",
    "def test(extension = \".jpeg\"):\n",
    "    try:\n",
    "        totalTime=0\n",
    "        numTry=3\n",
    "        for _ in range(numTry):\n",
    "            totalTime+=timeit(lambda: display(Image(data=cv2.imencode(extension, frame)[1])), number=1)\n",
    "            clear_output(wait=True)\n",
    "        return totalTime/numTry, extension\n",
    "\n",
    "    except cv2.error as e: #usually \"unsupported file type\"\n",
    "        return (math.inf, extension, e)\n",
    "for x in sorted(\n",
    "    [test(extension) for extension in extensions], key=lambda x: x[0]\n",
    "): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jin-yee.chan\\Documents\\TaxtDetection\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jin-yee.chan/Documents/TaxtDetection/train.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m _, frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimencode(\u001b[39m'\u001b[39m\u001b[39m.jpeg\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jin-yee.chan/Documents/TaxtDetection/train.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Detect and draw bounding boxes around tag\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jin-yee.chan/Documents/TaxtDetection/train.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m bounding_box: \u001b[39mlist\u001b[39m[Coords] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdetect(frame, saveImagePath\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./draw.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jin-yee.chan/Documents/TaxtDetection/train.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m bounding_box:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jin-yee.chan/Documents/TaxtDetection/train.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     box\u001b[39m.\u001b[39mdrawBoundingBox(\u001b[39m\"\u001b[39m\u001b[39m./draw.jpg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m./draw.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\Documents\\TaxtDetection\\detect.py:51\u001b[0m, in \u001b[0;36mTrainYOLO.detect\u001b[1;34m(self, imgPath, saveImagePath)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(\u001b[39mself\u001b[39m, imgPath: \u001b[39mstr\u001b[39m, saveImagePath: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Coords]:\n\u001b[1;32m---> 51\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(imgPath)\n\u001b[0;32m     52\u001b[0m     bounding_box: \u001b[39mlist\u001b[39m[Coords] \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:98\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:239\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m'\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39m# Preprocess\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 259\u001b[0m     im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(im0s)\n\u001b[0;32m    261\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:122\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    120\u001b[0m not_tensor \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(im, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m not_tensor:\n\u001b[1;32m--> 122\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_transform(im))\n\u001b[0;32m    123\u001b[0m     im \u001b[39m=\u001b[39m im[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))  \u001b[39m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(im)  \u001b[39m# contiguous\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:151\u001b[0m, in \u001b[0;36mBasePredictor.pre_transform\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    149\u001b[0m same_shapes \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(x\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m im[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m im)\n\u001b[0;32m    150\u001b[0m letterbox \u001b[39m=\u001b[39m LetterBox(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz, auto\u001b[39m=\u001b[39msame_shapes \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpt, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstride)\n\u001b[1;32m--> 151\u001b[0m \u001b[39mreturn\u001b[39;00m [letterbox(image\u001b[39m=\u001b[39;49mx) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m im]\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:151\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    149\u001b[0m same_shapes \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(x\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m im[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m im)\n\u001b[0;32m    150\u001b[0m letterbox \u001b[39m=\u001b[39m LetterBox(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz, auto\u001b[39m=\u001b[39msame_shapes \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpt, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstride)\n\u001b[1;32m--> 151\u001b[0m \u001b[39mreturn\u001b[39;00m [letterbox(image\u001b[39m=\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m im]\n",
      "File \u001b[1;32mc:\\Users\\jin-yee.chan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\augment.py:668\u001b[0m, in \u001b[0;36mLetterBox.__call__\u001b[1;34m(self, labels, image)\u001b[0m\n\u001b[0;32m    665\u001b[0m     new_shape \u001b[39m=\u001b[39m (new_shape, new_shape)\n\u001b[0;32m    667\u001b[0m \u001b[39m# Scale ratio (new / old)\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(new_shape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m shape[\u001b[39m0\u001b[39m], new_shape[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m shape[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    669\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaleup:  \u001b[39m# only scale down, do not scale up (for better val mAP)\u001b[39;00m\n\u001b[0;32m    670\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(r, \u001b[39m1.0\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from detect import TrainYOLO\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "display_handle=display(None, display_id=True)\n",
    "model = TrainYOLO(\"best.pt\")\n",
    "try:\n",
    "    while True:\n",
    "        _, frame = video.read()\n",
    "        frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        # Detect and draw bounding boxes around tag\n",
    "        bounding_box: list[Coords] = model.detect(frame, saveImagePath=\"./draw.jpg\")\n",
    "        for box in bounding_box:\n",
    "            box.drawBoundingBox(\"./draw.jpg\", \"./draw.jpg\")\n",
    "        break\n",
    "        # Recognize tag and display text\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    video.release()\n",
    "    display_handle.update(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropedImgPath = './crop.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "print(pytesseract.image_to_string(Image.open(cropedImgPath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = reader.readtext(x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from doctr.models import ocr_predictor\n",
    "# from doctr.io import DocumentFile\n",
    "# model = ocr_predictor('db_resnet50', 'crnn_mobilenet_v3_small', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropedImgPath = './crop.jpg'\n",
    "# image = DocumentFile.from_images(cropedImgPath)\n",
    "# out = model(image)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytesseract\n",
    "# from PIL import Image\n",
    "# print(pytesseract.image_to_string(Image.open(cropedImgPath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
